{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Data From Kaggle\n",
        "\n",
        "**Note:** *Download your Api Key from Kaggle to download the file directly in google colab or any cloud resources.*\n",
        "\n",
        "*Else, download the dataset locallly and upload in google colab or any cloud resources you are using.*\n",
        "\n",
        "*I'll be using Google Colab for this project.*"
      ],
      "metadata": {
        "id": "ccNthVbFq6a9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhvJW20GCw7q"
      },
      "outputs": [],
      "source": [
        "!pip -q install kaggle\n",
        "!pip -q install python_speech_features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the API token.\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move the uploaded file to the appropriate location.\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "lm3MMvF9C1Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d carlthome/gtzan-genre-collection"
      ],
      "metadata": {
        "id": "V8XtTNxpDedh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lah"
      ],
      "metadata": {
        "id": "TiqY07JvDj5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q gtzan-genre-collection.zip"
      ],
      "metadata": {
        "id": "WMEP6bh6DflY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "K6QXparfr2ZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import altair as alt\n",
        "\n",
        "from python_speech_features import mfcc\n",
        "import librosa\n",
        "\n",
        "import joblib"
      ],
      "metadata": {
        "id": "6f9qA8i9n7sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading, Preprocessing and Feature Extraction"
      ],
      "metadata": {
        "id": "q7lpaoe0r4sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract features from audio files\n",
        "def extract_features(file_path, mfcc=True, chroma=True, mel=True):\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        audio, sr = librosa.load(file)\n",
        "        features = []\n",
        "        if mfcc:\n",
        "            mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20).T, axis=0)\n",
        "            features.extend(mfccs)\n",
        "        if chroma:\n",
        "            chroma = np.mean(librosa.feature.chroma_stft(y=audio, sr=sr).T,axis=0)\n",
        "            features.extend(chroma)\n",
        "        if mel:\n",
        "            mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sr).T,axis=0)\n",
        "            features.extend(mel)\n",
        "    return features\n",
        "\n",
        "# Function to load dataset\n",
        "def load_dataset(data_path):\n",
        "    labels = []\n",
        "    features = []\n",
        "    for root, dirs, files in os.walk(data_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".au\"):\n",
        "                file_path = os.path.join(root, file)\n",
        "                genre = file.split(\".\")[0]\n",
        "                features.append(extract_features(file_path))\n",
        "                labels.append(genre)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Define paths\n",
        "data_path = \"genres\"\n",
        "# Load dataset\n",
        "features, labels = load_dataset(data_path)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Dataset loaded successfully!\")"
      ],
      "metadata": {
        "id": "LRYYqB_YDpR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of X_train : {X_train.shape}\")\n",
        "print(f\"Shape of y_train : {y_train.shape}\")\n",
        "print(f\"Shape of X_test : {X_test.shape}\")\n",
        "print(f\"Shape of y_test : {y_test.shape}\")\n",
        "print(f\"Sample X_train : {X_train[0]}\")\n",
        "print(f\"Sample y_train : {y_train[0]}\")\n",
        "print(f\"Sample y_train : {y_train[0]}\")\n",
        "print(f\"Checking dataset imbalance : {np.array(np.unique(y_train, return_counts=True)).T}\")\n"
      ],
      "metadata": {
        "id": "0FwjBirjEKPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "lbsfDb2XmIrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Development and Testing"
      ],
      "metadata": {
        "id": "ZD0vNuScsE36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize classifiers\n",
        "classifiers = {\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"SVM\": SVC(kernel='linear', random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"Naive Bayes\": GaussianNB()\n",
        "}\n",
        "\n",
        "# Train and evaluate classifiers\n",
        "trained_models = {}\n",
        "results = {}\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(X_train_scaled, y_train)\n",
        "    trained_models[name] = clf\n",
        "    y_pred = clf.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    results[name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}\n",
        "\n",
        "# Convert results to DataFrame for easier plotting\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.round(2)"
      ],
      "metadata": {
        "id": "8J8Ec2bnn3Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melt the DataFrame to have a long format suitable for Altair\n",
        "results_melted = results_df.reset_index().melt(id_vars='index', var_name='Metric', value_name='Value')\n",
        "\n",
        "# Determine the maximum value across all metrics\n",
        "max_value = results_melted['Value'].max() * 1.2\n",
        "\n",
        "# Plotting using Altair\n",
        "bars = alt.Chart(results_melted).mark_bar().encode(\n",
        "    x=alt.X('index:N', title='Classifier'),\n",
        "    y=alt.Y('Value:Q', title='Value', scale=alt.Scale(domain=[0, max_value])),\n",
        "    color='Metric:N',\n",
        "    tooltip=['index', 'Metric', 'Value']\n",
        ").properties(\n",
        "    width=300,\n",
        "    height=300\n",
        ")\n",
        "\n",
        "# Define text inside the bars\n",
        "text = bars.mark_text(\n",
        "    align='center',\n",
        "    baseline='middle',\n",
        "    dx=0,  # Nudges text to right so it doesn't appear on top of the bar\n",
        "    dy=-5,  # Nudges text upward\n",
        ").encode(\n",
        "    text='Value:Q'\n",
        ")\n",
        "\n",
        "# Combine bars and text\n",
        "chart = (bars + text)\n",
        "\n",
        "# Arrange two plots per row\n",
        "chart.facet(\n",
        "    column='Metric:N',\n",
        "    columns=2\n",
        ").properties(title='Performance Metrics of Different Classifiers').interactive()"
      ],
      "metadata": {
        "id": "cYqd0uU8pd0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Scalers and Models for Deployment"
      ],
      "metadata": {
        "id": "-cGb_3pYu9XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save scaler\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "\n",
        "# Save trained models\n",
        "models_dir = 'models'\n",
        "if not os.path.exists(models_dir):\n",
        "    os.makedirs(models_dir)\n",
        "\n",
        "for name, clf in trained_models.items():\n",
        "    model_filename = f\"{name.lower().replace(' ', '_')}_model.pkl\"\n",
        "    joblib.dump(clf, os.path.join(models_dir, model_filename))\n",
        "\n",
        "print(\"Scalers and models saved successfully!\")"
      ],
      "metadata": {
        "id": "XZlVCM5etnHn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}